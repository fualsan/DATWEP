{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a97ce94-4692-441c-afc3-0bdaab388bc0",
   "metadata": {},
   "source": [
    "# Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1e9e0f-ca24-4905-ae82-2552eb8e195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6052/3281021787.py:18: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from character_mappings import *\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df20cf9-8ca9-47e3-b129-494a1318a079",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "* MODELS_FOLDER\n",
    "  *  Pre trained models folder\n",
    "* RESULTS_AND_HISTORY_FOLDER\n",
    "  * History and results dicts save folder\n",
    "* CL\n",
    "  * Curriculum learning stuff \n",
    "* ALPHA\n",
    "  * Balance between vqa and seg loss (should be between 0,1)\n",
    "  * Higher -> more vqa, less seg\n",
    "* ADJUST_ALPHA\n",
    "  * Dynamically adjust ALPHA using DATAP\n",
    "* EPSILON\n",
    "  * Learning rate for ALPHA adjustment (see UPDATING ALPHA in training())\n",
    "* ADJUST_WEIGHTS\n",
    "  * Dynamically adjust class weights using DAWEP\n",
    "* ADJUST_WEIGHTS_LR\n",
    "  * Learning rate for weight adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e2400b-0f15-4ca1-be42-1eeaee1b599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'SEED': 99999, # for reproducibility\n",
    "    'MODELS_FOLDER': 'saved_models/',\n",
    "    'RESULTS_AND_HISTORY_FOLDER': 'results_and_history/',\n",
    "    'DATASET': {}, # Filled in a few cells below\n",
    "    'SAVE_MODEL': False, # Save model at the end of the training\n",
    "    'BATCH_SIZE': 8,\n",
    "    'EPOCHS': 25,\n",
    "    'SHUFFLE': False,\n",
    "    'PIN_MEMORY': True,\n",
    "    'NUM_WORKERS': 16,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'MOMENTUM': 0.9, # For SGD optimizer\n",
    "    'LR_SCHEDULING': {\n",
    "        'TYPE': 'StepLR',\n",
    "        'STEP_SIZE': 3,\n",
    "        'GAMMA': 0.95,\n",
    "    },\n",
    "    'CL': { \n",
    "        'ADJUST_ALPHA': True,\n",
    "        'ALPHA': 0.5,\n",
    "        'EPSILON': 0.002, \n",
    "        'ADJUST_WEIGHTS': True,\n",
    "        'ADJUST_WEIGHTS_LR': 0.001\n",
    "    },\n",
    "    'IMAGE': {\n",
    "        'RES': 200,\n",
    "        'RES_UNET_SIZES': {\n",
    "            128: (64, 32, 16),\n",
    "            200: (100, 50, 25)\n",
    "        },\n",
    "        'IN_CHANNELS': 3,\n",
    "        'FEATURES': 64,\n",
    "        'UNET_FEATURES_DIM': (64, 128, 256), # Used by VQAClassifier to pool the incoming features\n",
    "        'RELU_INPLACE': True,\n",
    "    },\n",
    "    'TEXT': {\n",
    "        'N_CHAR_TOKENS': len(char2idx.keys()), # char2idx is from character_mappings.py\n",
    "        'EMB_DIM': 8,\n",
    "        'POS_EMB_DROPOUT': 0.0,\n",
    "        'FREEZE_UNET': False,\n",
    "        'TEXT_DROPOUT_PROB': 0.2, # VQAClassifier text features dropout rate # 0.2\n",
    "        'COMBINED_DROPOUT_PROB': 0.2, # VQAClassifier combined features dropout rate # 0.2\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7000c78-64cf-479f-8cc8-293ff6bf659a",
   "metadata": {},
   "source": [
    "### Add Segmentation Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3670c88e-9b9b-4eb4-b547-14fd448e4df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 classes in total for segmentation\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/pdf/2012.02951.pdf\n",
    "SEG_CLASS_NAMES = [\n",
    "    'Background',\n",
    "    'Building-flooded',\n",
    "    'Building-non-flodded',\n",
    "    'Road-flooded',\n",
    "    'Road-non-flooded',\n",
    "    'Water',\n",
    "    'Tree',\n",
    "    'Vehicle',\n",
    "    'Pool',\n",
    "    'Grass'\n",
    "]\n",
    "\n",
    "\n",
    "hyperparameters['IMAGE']['SEG_CLASS_NAMES'] = SEG_CLASS_NAMES\n",
    "hyperparameters['IMAGE']['NUM_SEG_CLASSES'] = len(SEG_CLASS_NAMES)\n",
    "\n",
    "print(f\"{hyperparameters['IMAGE']['NUM_SEG_CLASSES']} classes in total for segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bdb44-182a-4f55-b8e4-6ea4e7c833cf",
   "metadata": {},
   "source": [
    "### Make the Save Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c26b35-5f6e-47b4-80c9-0321b75b78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(hyperparameters['MODELS_FOLDER'], exist_ok=True)\n",
    "os.makedirs(hyperparameters['RESULTS_AND_HISTORY_FOLDER'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20de1a-1eda-402b-af44-3afc855e55c3",
   "metadata": {},
   "source": [
    "# Seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb40fc7-6b30-422d-acf9-0dd71f16d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(hyperparameters['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab952d-dc72-4d78-aa3e-dd3375c0b0f2",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c76781-29f5-4188-bc70-4354adbe973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from custom_datasets.cached import FloodnetCachedDataset\n",
    "from custom_datasets.non_cached import FloodnetDataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e5944ec-229b-4add-8aaa-216d4496105f",
   "metadata": {},
   "source": [
    "full_train_dataset = FloodnetCachedDataset(\n",
    "    #full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}_no_pad.npy'),\n",
    "    #full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}.npy'),\n",
    "    full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}_optimized.npy'),\n",
    "    char2idx=char2idx,\n",
    "    idx2char=idx2char,\n",
    "    max_question_len=MAX_QUESTION_LEN\n",
    ")\n",
    "\n",
    "ds_sample = full_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d4995-b605-4d58-8f5a-6edfa235ab42",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "* NOTE: You can set your own data root from below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da03f515-5b19-44a1-ad70-7049ce885ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters['DATASET']['DATA_ROOT'] = '../data/'\n",
    "hyperparameters['DATASET']['IMAGES_ROOT'] = os.path.join(hyperparameters['DATASET']['DATA_ROOT'], 'FloodNet/track2_vqa/Images/')\n",
    "hyperparameters['DATASET']['QUESTIONS_ROOT'] = os.path.join(hyperparameters['DATASET']['DATA_ROOT'], 'FloodNet/track2_vqa/Questions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7563f157-db16-4fbb-95cc-bda6abbc8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters['DATASET']['TRAIN_IMAGES_PATH'] = os.path.join(hyperparameters['DATASET']['IMAGES_ROOT'], 'Train_Image')\n",
    "\n",
    "# BELOW IS NOT REQUIRED SINCE WE DIVIDE THE TRAIN ITSELF\n",
    "#valid_images_path = os.path.join(hyperparameters['DATASET']['IMAGES_ROOT'], 'Valid_Image')\n",
    "#test_images_path = os.path.join(hyperparameters['DATASET']['IMAGES_ROOT'], 'Test_Image')\n",
    "\n",
    "# From segmentation track\n",
    "hyperparameters['DATASET']['MASK_IMAGES_PATH'] = os.path.join(hyperparameters['DATASET']['DATA_ROOT'], 'FloodNet/track1_seg/train-label-img')\n",
    "\n",
    "hyperparameters['DATASET']['TRAIN_QUESTIONS_PATH'] = os.path.join(hyperparameters['DATASET']['QUESTIONS_ROOT'], 'Training Question.json')\n",
    "hyperparameters['DATASET']['VALID_QUESTIONS_PATH'] = os.path.join(hyperparameters['DATASET']['QUESTIONS_ROOT'], 'Valid Question.json')\n",
    "hyperparameters['DATASET']['TEST_QUESTIONS_PATH'] = os.path.join(hyperparameters['DATASET']['QUESTIONS_ROOT'], 'Test_Question.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49ea31e-efdb-4374-931d-a30512132a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4511"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(hyperparameters['DATASET']['TRAIN_QUESTIONS_PATH'], 'r') as f:\n",
    "    train_questions = json.load(f)\n",
    "    \n",
    "train_ds_len = len(train_questions)\n",
    "train_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27348b28-9ee3-4fb7-8d49-9a018d61386e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1415"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(hyperparameters['DATASET']['VALID_QUESTIONS_PATH'], 'r') as f:\n",
    "    valid_questions = json.load(f)\n",
    "    \n",
    "valid_ds_len = len(valid_questions)\n",
    "valid_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c4a42c-930a-4ca6-81d4-d93ae7fdc907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1429"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(hyperparameters['DATASET']['TEST_QUESTIONS_PATH'], 'r') as f:\n",
    "    test_questions = json.load(f)\n",
    "    \n",
    "test_ds_len = len(test_questions)\n",
    "test_ds_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3675fbf-f340-4af5-b6a8-74e6b282ba99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Image_ID': '10165.JPG',\n",
       " 'Question': 'What is the overall condition of the given image?',\n",
       " 'Ground_Truth': 'flooded',\n",
       " 'Question_Type': 'Condition_Recognition'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff8406-dbf8-4755-abc7-7c55ba162e38",
   "metadata": {},
   "source": [
    "# NOTE: VALID AND TEST SETS DON'T HAVE GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a6d9fc-47be-4510-ba6f-38507e99aa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Image_ID': '10169.JPG',\n",
       " 'Question': 'What is the overall condition of the given image?',\n",
       " 'Question_Type': 'Condition_Recognition'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_questions['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb1f6718-f49f-4393-86fc-71a947438a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Image_ID': '10163.JPG',\n",
       " 'Question': 'What is the overall condition of the given image?',\n",
       " 'Question_Type': 'Condition_Recognition'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_questions['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "048b7984-9e54-4b48-94f5-233fef28e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in valid_questions.items():\n",
    "    if 'Ground_Truth' in v.keys():\n",
    "        print(f'Found at: {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0120077-ae03-45e4-924c-141dac0fccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_type_counter_train = defaultdict(int)\n",
    "question_type_counter_valid = defaultdict(int)\n",
    "question_type_counter_test = defaultdict(int)\n",
    "\n",
    "keys_train = set()\n",
    "keys_valid = set()\n",
    "keys_test = set()\n",
    "\n",
    "for k, v in train_questions.items():\n",
    "    q_type = v['Question_Type']\n",
    "    question_type_counter_train[q_type] += 1\n",
    "    \n",
    "    for __key in v.keys():\n",
    "        keys_train.add(__key)\n",
    "\n",
    "for k, v in valid_questions.items():\n",
    "    q_type = v['Question_Type']\n",
    "    question_type_counter_valid[q_type] += 1\n",
    "    \n",
    "    for __key in v.keys():\n",
    "        keys_valid.add(__key)\n",
    "        \n",
    "for k, v in test_questions.items():\n",
    "    q_type = v['Question_Type']\n",
    "    question_type_counter_test[q_type] += 1\n",
    "    \n",
    "    for __key in v.keys():\n",
    "        keys_test.add(__key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b5f825d-799b-4cf7-b9f9-9a46599dd333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Condition_Recognition': 2315, 'Complex_Counting': 693, 'Simple_Counting': 636, 'Yes_No': 867})\n",
      "defaultdict(<class 'int'>, {'Condition_Recognition': 726, 'Simple_Counting': 197, 'Yes_No': 276, 'Complex_Counting': 216})\n",
      "defaultdict(<class 'int'>, {'Condition_Recognition': 728, 'Yes_No': 278, 'Complex_Counting': 222, 'Simple_Counting': 201})\n",
      "{'Ground_Truth', 'Question', 'Image_ID', 'Question_Type'}\n",
      "{'Question', 'Image_ID', 'Question_Type'}\n",
      "{'Question', 'Image_ID', 'Question_Type'}\n"
     ]
    }
   ],
   "source": [
    "print(question_type_counter_train)\n",
    "print(question_type_counter_valid)\n",
    "print(question_type_counter_test)\n",
    "\n",
    "print(keys_train)\n",
    "print(keys_valid)\n",
    "print(keys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "342999d6-dde9-4cc9-bbe5-8f5c58b2c0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_answers = defaultdict(int)\n",
    "\n",
    "for k, v in train_questions.items():\n",
    "    q_answer = v['Ground_Truth']\n",
    "    possible_answers[q_answer] += 1\n",
    "    \n",
    "hyperparameters['DATASET']['NUM_VQA_ANSWER_CLASSES'] = len(possible_answers.keys())\n",
    "hyperparameters['DATASET']['NUM_VQA_ANSWER_CLASSES'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0994b73-fe1f-48c0-b6a2-ebba79f0b714",
   "metadata": {},
   "source": [
    "# Lookup tables for possible types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae44c8d4-63c7-4368-92b9-4be1c6626d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All possible questions types mapped to an integer\n",
    "#question_type_dict = {k:i for i, k in enumerate(question_type_counter_train.keys())}\n",
    "hyperparameters['DATASET']['QUESTION_TYPE_DICT'] = {k:i for i, k in enumerate(question_type_counter_train.keys())}\n",
    "# All possible answers mapped to an integer\n",
    "#answers_dict =  {k:i for i, k in enumerate(possible_answers.keys())}\n",
    "hyperparameters['DATASET']['ANSWERS_DICT'] = {k:i for i, k in enumerate(possible_answers.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e705023-9c69-419c-a652-a7a9f3d80f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters['DATASET']['ANSWERS_DICT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb47e5f-a32b-49e1-915d-05ecf84b5a78",
   "metadata": {},
   "source": [
    "# NOTE: IS IT A GOOD IDEA TO SPLIT TRAIN \n",
    "## USE CL TO MAKE SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e7679d5-a5f0-42ea-b1da-3ec1a4e997a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Image_ID': '10165.JPG',\n",
       "  'Question': 'What is the overall condition of the given image?',\n",
       "  'Ground_Truth': 'flooded',\n",
       "  'Question_Type': 'Condition_Recognition'},\n",
       " {'Image_ID': '10166.JPG',\n",
       "  'Question': 'What is the overall condition of the given image?',\n",
       "  'Ground_Truth': 'flooded',\n",
       "  'Question_Type': 'Condition_Recognition'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list = list(train_questions.values())\n",
    "questions_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "954c37c0-fbdb-4735-b782-6df97b83913e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DATA_ROOT', 'IMAGES_ROOT', 'QUESTIONS_ROOT', 'TRAIN_IMAGES_PATH', 'MASK_IMAGES_PATH', 'TRAIN_QUESTIONS_PATH', 'VALID_QUESTIONS_PATH', 'TEST_QUESTIONS_PATH', 'NUM_VQA_ANSWER_CLASSES', 'QUESTION_TYPE_DICT', 'ANSWERS_DICT'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters['DATASET'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99b4c415-aaef-4122-9a6e-830d5d266c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average question length: 51\n",
      "Maximum question length: 70\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "full_train_dataset = FloodnetCachedDataset(\n",
    "    #full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}_no_pad.npy'),\n",
    "    #full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}.npy'),\n",
    "    full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}_optimized.npy'),\n",
    "    char2idx=char2idx,\n",
    "    idx2char=idx2char,\n",
    "    max_question_len=MAX_QUESTION_LEN\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "img_transforms = A.Compose([\n",
    "    A.Resize(width=hyperparameters['IMAGE']['RES'], height=hyperparameters['IMAGE']['RES']),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "full_train_dataset = FloodnetDataset(\n",
    "    train_images_path=hyperparameters['DATASET']['TRAIN_IMAGES_PATH'],\n",
    "    mask_images_path=hyperparameters['DATASET']['MASK_IMAGES_PATH'],\n",
    "    img_transforms=img_transforms,\n",
    "    num_segmentation_classes=hyperparameters['IMAGE']['NUM_SEG_CLASSES'],\n",
    "    questions_list=questions_list,\n",
    "    answers_dict=hyperparameters['DATASET']['ANSWERS_DICT'],\n",
    "    char2idx=char2idx, # from character_mappings.py\n",
    "    idx2char=idx2char, # from character_mappings.py\n",
    "    question_type_dict=hyperparameters['DATASET']['QUESTION_TYPE_DICT'], \n",
    "    max_question_len='auto',\n",
    "    use_average_len=False,\n",
    ")\n",
    "\n",
    "\n",
    "ds_sample = full_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca9354-57c3-4cf8-b2a0-b85ac3d0a514",
   "metadata": {},
   "source": [
    "### Save Maximum Question Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44e829ea-af9a-4f5f-a7a7-6a7084c070c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters['DATASET']['MAX_QUESTION_LEN'] = full_train_dataset.max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b5d0d21-8d2a-401e-aa60-cf0c8c6ae5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 200, 200])\n",
      "torch.Size([10, 200, 200])\n",
      "torch.Size([70])\n",
      "torch.Size([70])\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(ds_sample[0].shape)\n",
    "print(ds_sample[1].shape)\n",
    "print(ds_sample[2].shape)\n",
    "print(ds_sample[3].shape)\n",
    "print(ds_sample[4])\n",
    "print(ds_sample[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39220f58-5b3e-491f-8611-be32cc6044ba",
   "metadata": {},
   "source": [
    "### Cached version is no longer in use due to the memory issues"
   ]
  },
  {
   "cell_type": "raw",
   "id": "469bb33a-2157-47fd-8864-531eec4b9d99",
   "metadata": {},
   "source": [
    "full_train_dataset = FullCachedDataset(\n",
    "    #full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}_no_pad.npy'),\n",
    "    #full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}.npy'),\n",
    "    full_cache_file=os.path.join(CACHE_FOLDER, f'FloodNet_vqa_seg_combined_train_{RES}_optimized.npy'),\n",
    "    char2idx=char2idx,\n",
    "    idx2char=idx2char,\n",
    "    max_question_len=MAX_QUESTION_LEN\n",
    ")\n",
    "\n",
    "# NOTE: Some of the data points are discarded in caching procedure (check error count)\n",
    "len(full_train_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5943304-f439-4974-b3cd-4e94a84f914f",
   "metadata": {},
   "source": [
    "sample_full = full_train_dataset[0]\n",
    "len(sample_full)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bc7f50a-7df7-4efd-afec-7170aef48051",
   "metadata": {},
   "source": [
    "print(sample_full[0].shape)\n",
    "print(sample_full[1].shape)\n",
    "print(sample_full[2].shape)\n",
    "print(sample_full[3].shape)\n",
    "print(sample_full[4])\n",
    "print(sample_full[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d079c2d-6bc8-4054-8c43-96fb8515597c",
   "metadata": {},
   "source": [
    "### Get all answers before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f4f7a7c-c85d-4fcd-9fe6-82a9754418a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4511/4511 [00:00<00:00, 5338743.04it/s]\n"
     ]
    }
   ],
   "source": [
    "y_full = []\n",
    "\n",
    "for i in tqdm(range(len(full_train_dataset))):\n",
    "    # This is time consuming\n",
    "    #y_full.append(full_train_dataset[i][-1])\n",
    "    y_full.append(full_train_dataset.get_only_answers(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f159ce-4205-44bd-9339-7542b8d4afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_indexes = list(range(len(y_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dfc2409-f002-4d92-8d3f-09ef8ba65c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4511"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f14fe2-1058-4fa5-9067-5f2ad6e615b1",
   "metadata": {},
   "source": [
    "### NOTE: stratify can be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "944b2f45-f179-4bd6-9d12-cbd66a0a854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset_idx, test_dataset_idx, y_train, y_test = train_test_split(full_indexes, y_full, test_size=0.30, random_state=hyperparameters['SEED'], stratify=y_full)\n",
    "#test_dataset_idx, val_dataset_idx, y_test, y_val = train_test_split(test_dataset_idx, y_test, test_size=0.50, random_state=hyperparameters['SEED'], stratify=y_test)\n",
    "\n",
    "train_dataset_idx, test_dataset_idx, y_train, y_test = train_test_split(full_indexes, y_full, test_size=0.30, random_state=hyperparameters['SEED'])\n",
    "test_dataset_idx, val_dataset_idx, y_test, y_val = train_test_split(test_dataset_idx, y_test, test_size=0.50, random_state=hyperparameters['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb603d-e2f5-4133-bd6b-e797b32426a5",
   "metadata": {},
   "source": [
    "### Save original IDX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d4dc818-6861-4d69-84b4-af9bf06c101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters['DATASET']['TRAIN_DATASET_IDX'] = train_dataset_idx\n",
    "hyperparameters['DATASET']['VAL_DATASET_IDX'] = val_dataset_idx\n",
    "hyperparameters['DATASET']['TEST_DATASET_IDX'] = test_dataset_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6449e7c3-82fe-46bd-a12e-4eaf586d836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions_list = [questions_list[i] for i in train_dataset_idx] \n",
    "val_questions_list = [questions_list[i] for i in val_dataset_idx] \n",
    "test_questions_list = [questions_list[i] for i in test_dataset_idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55e9ec6a-b989-409e-9413-8c4a46d8d5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train: 3157 Val: 677 Test: 677 Question/Answer Pairs\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FloodnetDataset(\n",
    "    train_images_path=hyperparameters['DATASET']['TRAIN_IMAGES_PATH'],\n",
    "    mask_images_path=hyperparameters['DATASET']['MASK_IMAGES_PATH'],\n",
    "    img_transforms=img_transforms,\n",
    "    num_segmentation_classes=hyperparameters['IMAGE']['NUM_SEG_CLASSES'],\n",
    "    questions_list=train_questions_list,\n",
    "    answers_dict=hyperparameters['DATASET']['ANSWERS_DICT'],\n",
    "    char2idx=char2idx, # from character_mappings.py\n",
    "    idx2char=idx2char, # from character_mappings.py\n",
    "    question_type_dict=hyperparameters['DATASET']['QUESTION_TYPE_DICT'], \n",
    "    max_question_len=hyperparameters['DATASET']['MAX_QUESTION_LEN'],\n",
    "    use_average_len=False,\n",
    ")\n",
    "\n",
    "val_dataset = FloodnetDataset(\n",
    "    train_images_path=hyperparameters['DATASET']['TRAIN_IMAGES_PATH'],\n",
    "    mask_images_path=hyperparameters['DATASET']['MASK_IMAGES_PATH'],\n",
    "    img_transforms=img_transforms,\n",
    "    num_segmentation_classes=hyperparameters['IMAGE']['NUM_SEG_CLASSES'],\n",
    "    questions_list=val_questions_list,\n",
    "    answers_dict=hyperparameters['DATASET']['ANSWERS_DICT'],\n",
    "    char2idx=char2idx, # from character_mappings.py\n",
    "    idx2char=idx2char, # from character_mappings.py\n",
    "    question_type_dict=hyperparameters['DATASET']['QUESTION_TYPE_DICT'], \n",
    "    max_question_len=hyperparameters['DATASET']['MAX_QUESTION_LEN'],\n",
    "    use_average_len=False,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = FloodnetDataset(\n",
    "    train_images_path=hyperparameters['DATASET']['TRAIN_IMAGES_PATH'],\n",
    "    mask_images_path=hyperparameters['DATASET']['MASK_IMAGES_PATH'],\n",
    "    img_transforms=img_transforms,\n",
    "    num_segmentation_classes=hyperparameters['IMAGE']['NUM_SEG_CLASSES'],\n",
    "    questions_list=test_questions_list,\n",
    "    answers_dict=hyperparameters['DATASET']['ANSWERS_DICT'],\n",
    "    char2idx=char2idx, # from character_mappings.py\n",
    "    idx2char=idx2char, # from character_mappings.py\n",
    "    question_type_dict=hyperparameters['DATASET']['QUESTION_TYPE_DICT'], \n",
    "    max_question_len=hyperparameters['DATASET']['MAX_QUESTION_LEN'],\n",
    "    use_average_len=False,\n",
    ")\n",
    "\n",
    "print(f'Length Train: {len(train_dataset)} Val: {len(val_dataset)} Test: {len(test_dataset)} Question/Answer Pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f2e9e-843c-4c06-a15f-74a00afd97f6",
   "metadata": {},
   "source": [
    "# Dynamic Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcbf1189-15c3-4edf-8ab7-c3339efc2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize each class weight to be 1\n",
    "dynamic_weights = {w: 1.0 for w in range(hyperparameters['DATASET']['NUM_VQA_ANSWER_CLASSES'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7242774-db55-4395-b1c1-462fc55cc024",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b84a1a2-198f-4e1f-a27e-a2416847a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=hyperparameters['BATCH_SIZE'],\n",
    "    shuffle=hyperparameters['SHUFFLE'],\n",
    "    num_workers=hyperparameters['NUM_WORKERS'],\n",
    "    pin_memory=hyperparameters['PIN_MEMORY'],\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=hyperparameters['BATCH_SIZE'],\n",
    "    shuffle=hyperparameters['SHUFFLE'],\n",
    "    num_workers=hyperparameters['NUM_WORKERS'],\n",
    "    pin_memory=hyperparameters['PIN_MEMORY'],\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=hyperparameters['BATCH_SIZE'],\n",
    "    shuffle=hyperparameters['SHUFFLE'],\n",
    "    num_workers=hyperparameters['NUM_WORKERS'],\n",
    "    pin_memory=hyperparameters['PIN_MEMORY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52074b40-218b-47ef-9ee4-55c53f6b29dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches, train: 395, val: 85, test: 85\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of batches, train: {len(train_dataloader)}, val: {len(val_dataloader)}, test: {len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f768896-5e9c-4d39-8e8b-021e1524e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15316115-2214-4276-8873-8579088ccc02",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8b07a0a-7484-4755-bac3-f52041a4cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.segmentation import UNet\n",
    "from models.vqa import VQAClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99d8d2cf-1b92-4990-b2a0-a2d8363c7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(\n",
    "    in_channels=hyperparameters['IMAGE']['IN_CHANNELS'], \n",
    "    n_classes=hyperparameters['IMAGE']['NUM_SEG_CLASSES'], \n",
    "    features=hyperparameters['IMAGE']['FEATURES'], \n",
    "    relu_inplace=hyperparameters['IMAGE']['RELU_INPLACE']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a903a6f-360f-4438-b9eb-cf456a64f413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N_CHAR_TOKENS': 42,\n",
       " 'EMB_DIM': 8,\n",
       " 'POS_EMB_DROPOUT': 0.0,\n",
       " 'FREEZE_UNET': False,\n",
       " 'TEXT_DROPOUT_PROB': 0.2,\n",
       " 'COMBINED_DROPOUT_PROB': 0.2}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5fcfb10-d70e-431f-a985-4ab37d7d39db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (image+text) features: 703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/hfuat/Documents/PUBLICATIONS/publication_1/DATWEP/models/vqa.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pe_tensor = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "model = VQAClassifier(\n",
    "    n_char_tokens=hyperparameters['TEXT']['N_CHAR_TOKENS'], \n",
    "    emb_dim=hyperparameters['TEXT']['EMB_DIM'], \n",
    "    pos_emb_dropout=hyperparameters['TEXT']['POS_EMB_DROPOUT'], \n",
    "    max_question_len=hyperparameters['DATASET']['MAX_QUESTION_LEN'], \n",
    "    n_vqa_answer_classes=hyperparameters['DATASET']['NUM_VQA_ANSWER_CLASSES'],\n",
    "    unet=unet,\n",
    "    unet_features_dim=hyperparameters['IMAGE']['UNET_FEATURES_DIM'],\n",
    "    image_features_pool_dim=hyperparameters['IMAGE']['RES_UNET_SIZES'][hyperparameters['IMAGE']['RES']],\n",
    "    freeze_unet=hyperparameters['TEXT']['FREEZE_UNET'],\n",
    "    text_dropout_prob=hyperparameters['TEXT']['TEXT_DROPOUT_PROB'],\n",
    "    combined_dropout_prob=hyperparameters['TEXT']['COMBINED_DROPOUT_PROB'],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0ec9d10-b720-4a85-812f-5b50b7f55115",
   "metadata": {},
   "source": [
    "#sample_answer_preds, sample_question_type_preds, sample_attn_scores, sample_final_output = model(sample_full[0].unsqueeze(0), sample_full[2].unsqueeze(0), sample_full[3].unsqueeze(0))\n",
    "sample_answer_preds, sample_attn_scores, sample_final_output = model(sample_full[0].unsqueeze(0), sample_full[2].unsqueeze(0), sample_full[3].unsqueeze(0))\n",
    "\n",
    "sample_answer_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc20c91-cc62-41ef-8b94-2675b2a6a4b0",
   "metadata": {},
   "source": [
    "### Total Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53a7bc14-8ad5-4f3b-85c6-22c7b5c49c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_utils import print_total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d53811e-9261-49f4-b0a3-1825af44150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 9,456,274\n",
      "Total trainable parameters: 9,456,274\n",
      "Total non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "print_total_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8dd5b3-9086-4b2f-a36d-549bf92edc10",
   "metadata": {},
   "source": [
    "# Optimizer, Loss and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39d45658-87f4-42e8-9eb2-2409cf18e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=hyperparameters['LEARNING_RATE'], \n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE, \n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "criterion_seg = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "criterion_answer = nn.CrossEntropyLoss()\n",
    "\n",
    "#scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=hyperparameters['LR_SCHEDULING']['STEP_SIZE'],\n",
    "    gamma=hyperparameters['LR_SCHEDULING']['GAMMA'],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e220d6-c934-41a2-9c0a-cd3ea361d4d7",
   "metadata": {},
   "source": [
    "# Test Dynamic Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a02ddbbd-c37f-481a-a01b-480904038a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import learning.dynamic_weights as dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05552e1a-1470-4faf-9aed-6ea09c39e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  image, masks, question_tokens, pad_mask, question_type_ids, answer_ids\n",
    "_sample = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e45851b-1ba0-4c4e-948b-bba8658cba46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 41])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__preds, __attn_scores, __final_output = model(_sample[0], _sample[2], _sample[3])\n",
    "\n",
    "__preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51f95112-79d0-4c11-8af5-93f1cd86266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4823 for target: 0\n",
      "Loss: 1.4112 for target: 1\n",
      "Loss: 0.4474 for target: 2\n",
      "Loss: 0.4893 for target: 8\n",
      "Loss: 0.8655 for target: 9\n"
     ]
    }
   ],
   "source": [
    "_ = dw.loss_per_class(\n",
    "    pred=__preds.detach(), # IF detach() is not used, backward function will remain\n",
    "    batch_size=__preds.shape[0], \n",
    "    target=_sample[5], \n",
    "    weight=dynamic_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd1135fc-1d25-4fc9-9c92-469c7ce6697d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0145)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.grad_wrt_weight(\n",
    "    pred=__preds.detach(), \n",
    "    target_wrt=2, \n",
    "    target=_sample[5], \n",
    "    batch_size=__preds.shape[0], \n",
    "    weight=dynamic_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "287f8583-550d-429f-aeac-4a5d27adfa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss PyTorch: 3.6957, My: 3.6957\n"
     ]
    }
   ],
   "source": [
    "loss_my = dw.CELoss(\n",
    "    pred=__preds.detach(), \n",
    "    batch_size=__preds.shape[0], \n",
    "    target=_sample[5], \n",
    "    weight=dynamic_weights, \n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "\n",
    "loss_torch = F.cross_entropy(\n",
    "    __preds.detach(), \n",
    "    _sample[5], \n",
    "    weight=torch.tensor(list(dynamic_weights.values())),\n",
    ")\n",
    "\n",
    "print(f'Loss PyTorch: {loss_torch:.4f}, My: {loss_my:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461b171-febb-4bd5-8dc3-1df5fddb2a79",
   "metadata": {},
   "source": [
    "### Adjust Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29707c2b-ff51-42c5-a8ef-dc6961da3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.curriculum_learning import adjust_weights"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4dea0a44-090c-4a55-9b7b-1ad99073ae8a",
   "metadata": {},
   "source": [
    "# NOTE: THIS MESSES WITH WEIGHTS BEFORE TRAINING!\n",
    "\n",
    "adjust_weights(\n",
    "    pred=__preds.detach(), \n",
    "    target=_sample[5], \n",
    "    weight=dynamic_weights, \n",
    "    lr_weight_adjust=2.0\n",
    ")\n",
    "\n",
    "dynamic_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663a83c-203f-4374-95dd-8823b2781204",
   "metadata": {},
   "source": [
    "# Save Hyperparameters Dict as a JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ae50869-e7b2-4d1b-9a01-a69e6796816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(hyperparameters['RESULTS_AND_HISTORY_FOLDER'], 'hyperparameters.json'), 'w') as f:\n",
    "    json.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae26206-cee8-44b2-90d8-eb7742f4011b",
   "metadata": {},
   "source": [
    "# Training\n",
    "### Segmentation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e75027f9-72c9-4628-b498-70d72cfc9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.segmentation_metrics import calculate_segmentation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a38b2d6-6c22-4e53-bd8b-9a2a77da400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.training import train_iter\n",
    "from learning.training import validate_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51e16a60-5e61-4a0f-b0b1-d0683adb12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records common metrics per epoch\n",
    "history = {\n",
    "    'train_loss': [], # Total loss\n",
    "    'val_loss': [],\n",
    "    'train_answer_acc': [],\n",
    "    'val_answer_acc': [],\n",
    "    #'train_question_type': [],\n",
    "    #'test_question_type': [],\n",
    "    'train_dice_score': [],\n",
    "    'val_dice_score': [],\n",
    "    'train_miou': [],\n",
    "    'val_miou': [],\n",
    "    'alpha': [],\n",
    "}\n",
    "\n",
    "\n",
    "# Records epoch_history per epoch\n",
    "# ex: inter_epoch_history['train'][0]['dice_score']\n",
    "inter_epoch_history = {\n",
    "    'train': [],\n",
    "    'val': [],\n",
    "}\n",
    "\n",
    "# Average past grades curriculum (IMPLEMENT MAYBE LATER ?)\n",
    "# Here vqa and seg defined in general, any metric can be applied\n",
    "past_avg_grades = {\n",
    "    'vqa': [],\n",
    "    'seg': []\n",
    "}\n",
    "\n",
    "## NEW: MODEL CHECKPOINT\n",
    "__model_save_path_checkpoint = os.path.join(\n",
    "    hyperparameters['MODELS_FOLDER'], \n",
    "    f\"VQAClassifier_{hyperparameters['IMAGE']['RES']}_checkpoint.pth\"\n",
    ")\n",
    "\n",
    "best_acc_epoch = -99\n",
    "\n",
    "def start_training(__ALPHA, __ADJUST_ALPHA, __EPSILON,  __ADJUST_WEIGHTS, _dynamic_weights, __ADJUST_WEIGHTS_LR, criterion_answer, criterion_seg):\n",
    "\n",
    "    global best_acc_epoch\n",
    "    model.to(device)\n",
    "    \n",
    "    best_acc = -1.0\n",
    "    \n",
    "    criterion_answer, criterion_seg = criterion_answer.to(device), criterion_seg.to(device)\n",
    "    \n",
    "    for epoch in range(1, hyperparameters['EPOCHS']+1):\n",
    "        \n",
    "        if hyperparameters['CL']['ADJUST_WEIGHTS']:\n",
    "            print(f'Current value of ALPHA: {__ALPHA:.2f}, EPSILON: {__EPSILON:.5f}, ADJUST_WEIGHTS_LR: {__ADJUST_WEIGHTS_LR:.5f}')\n",
    "        else:\n",
    "            print(f'Current value of ALPHA: {__ALPHA:.2f}, EPSILON: {__EPSILON:.5f}')\n",
    "\n",
    "            \n",
    "        # Add current alpha value to history\n",
    "        history['alpha'].append(__ALPHA)    \n",
    "            \n",
    "        # Train\n",
    "        train_metrics = train_iter(\n",
    "            epoch=epoch, \n",
    "            dataloader=train_dataloader, \n",
    "            model=model, \n",
    "            device=device,\n",
    "            optimizer=optimizer, \n",
    "            answer_loss_fn=criterion_answer, \n",
    "            seg_loss_fn=criterion_seg, \n",
    "            alpha=__ALPHA,\n",
    "            adjust_alpha=__ADJUST_ALPHA,\n",
    "            epsilon=__EPSILON,\n",
    "            _adjust_weights=__ADJUST_WEIGHTS, # not to be confused with the function\n",
    "            adjust_weights_fn=adjust_weights,\n",
    "            dynamic_weights=_dynamic_weights,\n",
    "            adjust_weights_lr=__ADJUST_WEIGHTS_LR,\n",
    "        )\n",
    "    \n",
    "        inter_epoch_history['train'].append(train_metrics['epoch_history'])\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = validate_iter(\n",
    "            epoch=epoch, \n",
    "            dataloader=val_dataloader, \n",
    "            model=model, \n",
    "            device=device, \n",
    "            answer_loss_fn=criterion_answer, \n",
    "            seg_loss_fn=criterion_seg, \n",
    "            alpha=__ALPHA,\n",
    "            dynamic_weights=_dynamic_weights\n",
    "        )\n",
    "\n",
    "        ### OVERWRITE ALPHA\n",
    "        if hyperparameters['CL']['ADJUST_ALPHA']:\n",
    "            __ALPHA = train_metrics['alpha']\n",
    "        ###################\n",
    "        \n",
    "        #avg_dice_test, miou_test = calculate_segmentation_metrics(val_dataloader, model)\n",
    "        #segmentation_metrics_val = calculate_segmentation_metrics(val_dataloader, model)\n",
    "        inter_epoch_history['val'].append(val_metrics['epoch_history'])\n",
    "\n",
    "                \n",
    "        avg_dice_train = train_metrics['avg_dice_score']\n",
    "        avg_dice_val = val_metrics['avg_dice_score']\n",
    "        miou_train = train_metrics['miou_score']\n",
    "        miou_val = val_metrics['miou_score']\n",
    "        \n",
    "\n",
    "        # Record metrics\n",
    "        history['train_loss'].append(train_metrics['avg_total_loss'])\n",
    "        history['val_loss'].append(val_metrics['avg_total_loss'])\n",
    "        history['train_answer_acc'].append(train_metrics['avg_answer_acc'])\n",
    "        history['val_answer_acc'].append(val_metrics['avg_answer_acc'])\n",
    "        # seg metrics\n",
    "        history['train_dice_score'].append(avg_dice_train)\n",
    "        history['val_dice_score'].append(avg_dice_val)\n",
    "        history['train_miou'].append(miou_train)\n",
    "        history['val_miou'].append(miou_val)\n",
    "                \n",
    "        # Print dice scores\n",
    "        print(f'Average dice score train: %{avg_dice_train:.2f}, val: %{avg_dice_val:.2f}, mIoU train: %{miou_train:.2f}, val: %{miou_val:.2f}')\n",
    "        \n",
    "        # Scheduler steps\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_metrics['avg_answer_acc'] > best_acc:\n",
    "            best_acc = val_metrics['avg_answer_acc']\n",
    "            print(f'New best validation answer acc: %{best_acc:.4f}')\n",
    "            best_acc_epoch = epoch\n",
    "            \n",
    "            torch.save({\n",
    "                'unet_state_dict': model.image_encoder.state_dict(),\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, __model_save_path_checkpoint)\n",
    "            print(f'Model checkpoint is saved to: {__model_save_path_checkpoint}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6617f9a0-4572-4bbc-b169-77cbb097cbd3",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0f6d2d9-9211-4deb-b3b3-443783ec4557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value of ALPHA: 0.50, EPSILON: 0.00200, ADJUST_WEIGHTS_LR: 0.00100\n",
      "train_iter current alpha: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train:   1%|▍                                                                                                 | 2/395 [00:09<31:47,  4.85s/batch, Loss: 2.1128, Acc answer: %6.25]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/mnt/c/Users/hfuat/Documents/PUBLICATIONS/publication_1/DATWEP/custom_datasets/non_cached.py\", line 97, in __getitem__\n    transformed = self.img_transforms(image=_image, masks=all_masks)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/albumentations/core/composition.py\", line 195, in __call__\n    self._check_args(**data)\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/albumentations/core/composition.py\", line 286, in _check_args\n    raise ValueError(\nValueError: Height and Width of image, mask or masks should be equal. You can disable shapes check by setting a parameter is_check_shapes=False of Compose class (do it only if you are sure about your data consistency).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__ALPHA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mALPHA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# keep the original alpha value \u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__ADJUST_ALPHA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADJUST_ALPHA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__EPSILON\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEPSILON\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__ADJUST_WEIGHTS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADJUST_WEIGHTS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dynamic_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__ADJUST_WEIGHTS_LR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADJUST_WEIGHTS_LR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_answer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_seg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_seg\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 60\u001b[0m, in \u001b[0;36mstart_training\u001b[0;34m(__ALPHA, __ADJUST_ALPHA, __EPSILON, __ADJUST_WEIGHTS, _dynamic_weights, __ADJUST_WEIGHTS_LR, criterion_answer, criterion_seg)\u001b[0m\n\u001b[1;32m     57\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(__ALPHA)    \n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer_loss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_loss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__ALPHA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43madjust_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__ADJUST_ALPHA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__EPSILON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_adjust_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__ADJUST_WEIGHTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# not to be confused with the function\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43madjust_weights_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjust_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dynamic_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43madjust_weights_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__ADJUST_WEIGHTS_LR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m inter_epoch_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_history\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/hfuat/Documents/PUBLICATIONS/publication_1/DATWEP/learning/training.py:46\u001b[0m, in \u001b[0;36mtrain_iter\u001b[0;34m(epoch, dataloader, model, device, optimizer, answer_loss_fn, seg_loss_fn, alpha, adjust_alpha, epsilon, _adjust_weights, adjust_weights_fn, dynamic_weights, adjust_weights_lr)\u001b[0m\n\u001b[1;32m     38\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m epoch_history \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseg_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvqa_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvqa_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;66;03m# SEG accuracy is in another function\u001b[39;00m\n\u001b[1;32m     44\u001b[0m }\n\u001b[0;32m---> 46\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# SEG\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# VQA\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datwep/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1326\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1325\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/mnt/c/Users/hfuat/Documents/PUBLICATIONS/publication_1/DATWEP/custom_datasets/non_cached.py\", line 97, in __getitem__\n    transformed = self.img_transforms(image=_image, masks=all_masks)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/albumentations/core/composition.py\", line 195, in __call__\n    self._check_args(**data)\n  File \"/home/lecturer/miniconda3/envs/datwep/lib/python3.11/site-packages/albumentations/core/composition.py\", line 286, in _check_args\n    raise ValueError(\nValueError: Height and Width of image, mask or masks should be equal. You can disable shapes check by setting a parameter is_check_shapes=False of Compose class (do it only if you are sure about your data consistency).\n"
     ]
    }
   ],
   "source": [
    "start_training(\n",
    "    __ALPHA=copy.deepcopy(hyperparameters['CL']['ALPHA']), # keep the original alpha value \n",
    "    __ADJUST_ALPHA=hyperparameters['CL']['ADJUST_ALPHA'], \n",
    "    __EPSILON=hyperparameters['CL']['EPSILON'],\n",
    "    __ADJUST_WEIGHTS=hyperparameters['CL']['ADJUST_WEIGHTS'], \n",
    "    _dynamic_weights=dynamic_weights, \n",
    "    __ADJUST_WEIGHTS_LR=hyperparameters['CL']['ADJUST_WEIGHTS_LR'], \n",
    "    criterion_answer=criterion_answer, \n",
    "    criterion_seg=criterion_seg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e047e21-d992-49ca-9018-9cd63892becb",
   "metadata": {},
   "source": [
    "### Load Best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5218de-c05f-48f3-aad8-3b66e30950aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.save({\n",
    "    'unet_state_dict': model.image_encoder.state_dict(),\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "}, __model_save_path_checkpoint)\n",
    "\"\"\"\n",
    "\n",
    "_checkpoint = torch.load(__model_save_path_checkpoint)\n",
    "unet.load_state_dict(_checkpoint['unet_state_dict'])\n",
    "model.load_state_dict(_checkpoint['model_state_dict'])\n",
    "print(f'Checkpoint sucessfully loaded from epoch: {best_acc_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a4310-28fd-491e-ab03-7c9cf3632ddd",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa1aa7-b87f-43ca-b679-5e3ddd7e8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters['SAVE_MODEL']:\n",
    "    __model_save_path = os.path.join(\n",
    "        hyperparameters['MODELS_FOLDER'], \n",
    "        f\"VQAClassifier_{hyperparameters['IMAGE']['RES']}_final.pth\"\n",
    "    )\n",
    "\n",
    "    torch.save({\n",
    "        'unet_state_dict': model.image_encoder.state_dict(),\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, __model_save_path)\n",
    "    print(f'Model is saved to: {__model_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b23428-6322-4f24-b6e1-3d83cd17b541",
   "metadata": {},
   "source": [
    "## List Final Weigths For Each Answer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742260c-4ebe-4a14-bcaa-080588547a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dynamic_weigths(__dynamic_weights):\n",
    "    # Convert tensors into integers\n",
    "    dynamic_weights_plain = {_class_id: _weight.item() for _class_id, _weight in dynamic_weights.items()}\n",
    "    save_path = os.path.join(hyperparameters['RESULTS_AND_HISTORY_FOLDER'], 'dynamic_weights.json')\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(dynamic_weights_plain, f)\n",
    "\n",
    "    print(f'Weights are saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9c421-d5d8-4f2f-a336-c5b42edf2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dynamic_weigths(dynamic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21498ab0-9a5a-461e-b8bd-6b84c27181e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict_reversed = {v:k for k,v in hyperparameters['DATASET']['ANSWERS_DICT'].items()}\n",
    "\n",
    "if hyperparameters['CL']['ADJUST_WEIGHTS']:\n",
    "    for _class_id, _weight in dynamic_weights.items():    \n",
    "        print(f'Answer: {answer_dict_reversed[_class_id]:<20} Weigth: {_weight.item():.4f}')\n",
    "else:\n",
    "    for _class_id, _weight in dynamic_weights.items():  \n",
    "        print(f'Answer: {answer_dict_reversed[_class_id]:<20} Weigth: {_weight:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31bd91-2ffb-4892-8214-bc4d8ecae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_epoch_history['train'][0].keys()\n",
    "print(len(inter_epoch_history['train'][0]['dice_score']))\n",
    "print(len(inter_epoch_history['train'][0]['vqa_loss']))\n",
    "print(len(inter_epoch_history['train'][0]['seg_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e56a4-f4bb-4147-8095-993f293198a9",
   "metadata": {},
   "source": [
    "### Plot of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3620bf-2835-4e2f-8c4e-c5b5e5cb18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for results directory saving \n",
    "save_dir = lambda filename: os.path.join(hyperparameters['RESULTS_AND_HISTORY_FOLDER'], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e4d0c-7d4a-4553-a23e-b7c2a63a0b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history['train_loss'])+1), history['train_loss'], c='b', label='training loss')\n",
    "plt.plot(range(1, len(history['val_loss'])+1), history['val_loss'], c='r', label='validation loss')\n",
    "#plt.title('Training and Validation VQA Answer Losses over Epochs')\n",
    "plt.ylabel('Answer Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.grid('on')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(save_dir('total_loss_plot.eps'), format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d32a0-d392-4e36-bceb-b9ef51674bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history['train_answer_acc'])+1), history['train_answer_acc'], c='b', label='training answer accuracy')\n",
    "plt.plot(range(1, len(history['val_answer_acc'])+1), history['val_answer_acc'], c='r', label='validation answer accuracy')\n",
    "#plt.title('Training and Validation VQA Answer Accuracies over Epochs')\n",
    "plt.ylabel('Answer Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.grid('on')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(save_dir('vqa_acc_plot.eps'), format='eps')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7539b8b-67db-4015-9c69-cc17f4ad3e96",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history['train_question_type'])+1), history['train_question_type'], c='b', label='training QT accuracy')\n",
    "plt.plot(range(1, len(history['val_question_type'])+1), history['val_question_type'], c='r', label='validation QT accuracy')\n",
    "plt.grid('on')\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ba8ce-015c-47b9-9070-a4aa4259ae7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history['train_dice_score'])+1), history['train_dice_score'], c='b', label='train dice score')\n",
    "plt.plot(range(1, len(history['val_dice_score'])+1), history['val_dice_score'], c='r', label='validation dice score')\n",
    "#plt.title('Training and Validation Dice Scores over Epochs')\n",
    "plt.ylabel('Dice Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.grid('on')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(save_dir('dice_score_plot.eps'), format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5141d9-0d93-4c6d-9d09-881034c4233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history['train_miou'])+1), history['train_miou'], c='b', label='train mIoU')\n",
    "plt.plot(range(1, len(history['val_miou'])+1), history['val_miou'], c='r', label='validation mIoU')\n",
    "#plt.title('Training and Validation mIoU over Epochs')\n",
    "plt.ylabel('mIoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.grid('on')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(save_dir('mIoU_plot.eps'), format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32209003-0759-4bf6-babb-7a1ae2b36f07",
   "metadata": {},
   "source": [
    "# Plot of Alpha History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38515d15-c224-449d-85ff-84d693c7ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history['alpha'])+1), history['alpha'], c='r', label='alpha')\n",
    "#plt.title('Alpha values over Epochs')\n",
    "plt.ylabel('Values')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xlim(1, len(history['alpha'])+1)\n",
    "plt.grid('on')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(save_dir('alpha.eps'), format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f481a3f-fd6b-4088-b520-6cb3f3aac2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(history['alpha'])+1), [h*100 for h in history['alpha']], c='r', label='alpha')\n",
    "plt.plot(range(1, len(history['val_answer_acc'])+1), history['val_answer_acc'], c='b', label='validation answer accuracy')\n",
    "plt.plot(range(1, len(history['val_miou'])+1), history['val_miou'], c='g', label='validation mIoU')\n",
    "#plt.title('Validation metrics for both VQA and segmentation with alpha values over Epochs\\n(Alpha values multiplied by 100)')\n",
    "plt.ylabel('Values')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xlim(1, len(history['alpha'])+1)\n",
    "plt.grid('on')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(save_dir('alpha_with_metrics.eps'), format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b247f-273b-4388-be01-d55327fcd180",
   "metadata": {},
   "source": [
    "### Save history as JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dce62d-2ec2-4e53-98d6-d0b08fefde03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(hyperparameters['RESULTS_AND_HISTORY_FOLDER'], 'history_dict.json'), 'w') as f:\n",
    "    json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0875d0-b20c-4de5-abd4-7e2b62ac9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE TESTING, IMPLEMENT LEARNING ALGORITHM\n",
    "# PASSED, IMPLEMENTED DYNAMIC WEIGHTS (GRADIENTS WRT INPUT IS SAVED FOR LATER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830975b9-ef37-4de2-aadf-d49cd5bd5ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc39c56-3394-43d4-b868-170dd89c28c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
